#path to directory with .gguf files. It will be mounted as /models/ inside containers
MODEL_DIR=
MODEL_GGUF=

#configuration for llama.cpp build. should match the host CUDA version and ARCH
UBUNTU_VERSION=22.04
CUDA_VERSION=12.6.0
CUDA_DOCKER_ARCH=86
